

Use this package to manage Azure Resources from within an R Session. This is not a full SDK just a collection of functions that should prove useful for a Data Scientist who needs to access and manage Azure Resources.

# Installation instructions

Install the development version of the package directly from GitHub with:

```r
# Install devtools
if(!require("devtools")) install.packages("devtools")
devtools::install_github("Microsoft/AzureSMR")
library(AzureSM)
```

The package depends on:

- `jsonlite`
- `httr`
- `XML`
- `base64enc`
- `plyr`


# Overview

AzureSMR provides an interface to manage resources on Microsoft Azure
The main functions address the following Azure Services:

- AzureBlob: List, Read and Write to Blob Services
- AzureResources: List, Create and Delete Azure Resource
- AzureVM: List, Start and Stop Azure VMs
- AzureHDI: List and Scale Azure HDInsight Clusters
- AzureHive: Run Hive queries against a HDInsight Cluster
- AzureSpark: List and create Spark jobs/Sessions against a HDInsight Cluster(Livy) - EXPERIMENTAL


For a detailed list of AzureSM functions and their syntax please refer to the Help pages.

## Getting Authorisation configured

To get started, please refer to the Authorisation tutorial. https://github.com/Microsoft/AzureSMR/blob/master/vignettes/Authentication.Rmd


## AzureContext

The AzureAPIs require lots of parameters to be managed. Rather than supplying all the paramters for each function call AzureSMR implements an AzureContext Variable which caches the last time a paramters is used so that it doesnt need to be repeatedly supplied.

To create an AzureContext variable

```r
sc <- CreateAzureContext()
```
To manually set AzureContext Variables use the SetAzureContext function and supply the AzureContext and the variables.

```r
SetAzureContext(sc,TID="{TID}",
                   CID="{CID}",
                   KEY="{KEY}")
```
To see the contents of an AzureContext use DumpAzureContext()
```
DumpAzureContext(sc)
```

## Authorisation

To get an Authorisation Token use AzureAuthenticate. Note this Token will time our after a period and therefore you need to run it again occasionally. TIP: Use AzureAuthenticate before a long running task.
AzureListSubscriptions() lists all the available subscriptions. If you only have one it sets the default Subscription in the AzureContext to that SubscriptionID.


```r
AzureAuthenticate(sc)

AzureAuthenticate(sc,TID="{TID}",CID="{CID}",KEY= "{KEY}")
SUBS<-AzureListSubscriptions(sc); 

```
## Manage resource Groups

```r
RGS <- AzureListRG(sc) ; View(RGS)

RS <- AzureListAllRecources(sc)

RS <- AzureListAllRecources(sc,Location="northeurope")

RS <- AzureListAllRecources(sc, Type="Microsoft.Sql/servers",Location="northeurope"); View(RS)

RS <- AzureListAllRecources(sc,ResourceGroup ="Analytics")
View(RS)

AzureCreateResourceGroup(sc,"testme","northeurope")

AzureDeleteResourceGroup(sc,"testme")

AzureListRG(sc)$Name

```

```
## [1] AnalyticsRG                                                             
## [2] BiData1RG                                                          
## [3] Analytics         
```

## Manage Virtual Machines
Use these functions to List, Start and Stop Virtual Machines (see templates for Creation)
To Create VMs please refer to Resource Templates below

```
AzureListVM(sc,ResourceGroup="AWHDIRG")
```

```
##            Name    Location                             Type    OS     State  Admin
## 1         DSVM1 northeurope Microsoft.Compute/virtualMachines Linux Succeeded alanwe
```

```
AzureStartVM(sc,VMName="DSVM1")
AzureStopVM(sc,VMName="DSVM1")

```

## Access Storage Blobs

In order to access Storage Blobs you need to have a key. Use AzureSAGetKey to get a Key or alternatively supply your own key. When you provide your own key you no longer need to use AzureAuthenticate() as the API uses a diferent authentication approach.

```
sKEY <- AzureSAGetKey(sc,ResourceGroup = "Analytics",StorageAccount = "analyticsfiles")
```

To List Containers in a Storage Account use azListContainers

```
AL <- azListContainers(sc, StorageAccount="analyticsfiles",Containers="Test")
```


To List Blobs in a container use AzureListSABlobs

```
AzureListSABlobs(sc,StorageAccount="analyticsfiles",Container = "test")
```

To Write a Blobs use AzurePutBlob

```
OUT<- AzurePutBlob(sc,StorageAccount="analyticsfiles",Container="test",Contents="Hello World",Blob="HELLO") 
```

To Read a Blob in a container use AzureGetBlob

```
OUT<- AzureGetBlob(sc,StorageAccount="analyticsfiles",Container="test",Blob="HELLO",Type="text") 
```


## Manage HDInsight Clusters

AzureSM can be used to manage Azure HDInsight clusters. To create clusters use Resource Templates (See below).
Also see functions for submitting Hive and Spark jobs.

Use AzureListHDI to list available Clusters

```
AzureListHDI(sc)
AzureListHDI(sc,ResourceGroup ="Analytics")

```

use AzureResizeHDI to resize a Clusters

```
AzureResizeHDI(sc,ResourceGroup="Analytics",ClusterName = "{HDIClusterName}", Role="workernode",Size=2)
```

```
## AzureResizeHDI: Request Submitted:  2016-06-23 18:50:57
## Resizing(R), Succeeded(S)
## RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR## RRRRRRRRRRRRRRRRRRS
## Finished Resizing Sucessfully:  2016-06-23 19:04:43
## Finished:  2016-06-23 19:04:43
##                                                                                                                        ## Information 
## " headnode ( 2 * Standard_D3_v2 ) workernode ( 5 * Standard_D3_v2 ) zookeepernode ( 3 * Medium ) edgenode0 ( 1 * Standard_D4_v2 )" 
```

## Resource Templates - Create Azure Resources

The easiest way to create resources on Azure is to use Azure Templates. To create Azure Resources such as HDInsight clusters there can a large quantity of parameters. Resource templates can be built be creating a resource in the Azure Portal and then going into Settings > Automation scripts. Example templates can be found at this URL https://github.com/Azure/AzureStack-QuickStart-Templates.

To create a resource using a template in AzureSM use AzureDeployTemplate. The Template and Paramters must be available in a public URL (Azure Blob). It may be worth getting the Azure Administrator to build a working template.

```
AzureDeployTemplate(sc,ResourceGroup="Analytics",DeplName="Deploy1",TemplateURL="{TEMPLATEURL}",ParamURL="{PARAMURL}")
```
```
## AzureDeployTemplate: Request Submitted:  2016-06-23 18:50:57
## Resizing(R), Succeeded(S)
## RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR## RRRRRRRRRRRRRRRRRRS
## Finished Deployed Sucessfully:  2016-06-23 19:04:43
## Finished:  2016-06-23 19:04:43
```

ADMIN TIP: If a deployment fails. Go to the Azure Portal and look at Actvity logs and look for failed deployments which should explain why the deployment failed.

## Hive Functions
These functions facilitate the use of hive jobs on a HDInsight Cluster

```
AzureHiveStatus(sc,ClusterName = "{hdicluster}",HDIAdmin = "admin", HDIPassword = "********")
AzureHiveSQL(sc, CMD="select * from airports", Path="wasb://{container}@{hdicluster}.blob.core.windows.net/")
stdout <- AzureGetBlob(sc, Container = "test", Blob = "stdout")
 
airports <- read.table(text=stdout,  sep="\t", header=TRUE,fill=TRUE)
adf <- data.frame(airports)
```


## Spark functions (EXPERIMENTAL)

AzureSM provides some functions that allow HDInsight Spark Sessions and jobs to be managed within an R Session

To Create a new Spark Session (Via Livy) use AzureSparkNewSession

```
AzureSparkNewSession(sc, ClusterName = "{hdicluster}", HDIAdmin = "admin", HDIPassword = "********",Kind="pyspark")
```

To view the status of sessions use AzureSparkListSessions

```
AzureSparkListSessions(sc,ClusterName = "{hdicluster}")
```

To send a command to the Spark Session use AzureSparkCMD. In this case it submits a Python routine

```
PYCMD <- '
from pyspark import SparkContext
from operator import add
import sys
from random import random
partitions = 1
n = 20000000 * partitions
def f(_):
  x = random() * 2 - 1
  y = random() * 2 - 1
  return 1 if x ** 2 + y ** 2 < 1 else 0
 
count = sc.parallelize(range(1, n + 1), partitions).map(f).reduce(add)
Pi = (4.0 * count / n)
print("Pi is roughly %f" % Pi)'                   # SAMPLE PYSPARK SCRIPT TO CALCULATE PI
 
AzureSparkCMD(sc,CMD=PYCMD,SessionID="5")
```

```
## [1] "Pi is roughly 3.140285"
```

Check Session variables are retained
```
AzureSparkCMD(sc, ClusterName = "{hdicluster}",CMD="print Pi",SessionID="5")
```

```
#[1] "3.1422"
```

